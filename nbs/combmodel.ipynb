{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cProfile import run\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime,timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "import pandas\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import wandb\n",
    "import logging\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger, WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import os\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "wandb_logger = lambda dir, version: WandbLogger(\n",
    "    name=\"wandb\", save_dir=dir, version=version\n",
    ")\n",
    "csvlogger = lambda dir, version: CSVLogger(dir, name=\"csvlogs\", version=version)\n",
    "tblogger = lambda dir, version: TensorBoardLogger(dir, name=\"tblogs\", version=version)\n",
    "\n",
    "def get_loggers(dir,version,lis=[\"csv\"]):\n",
    "    lgrs = []\n",
    "    if \"wandb\" in lis:\n",
    "        lgrs.append(wandb_logger(dir, version))\n",
    "    if \"csv\" in lis:\n",
    "        lgrs.append(csvlogger(dir, version))\n",
    "    if \"tb\" in lis:\n",
    "        lgrs.append(tblogger(dir, version))\n",
    "    return lgrs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_dir =Path('/common/home/vk405/Projects/EnergyLab/Solar-forcast/Data/')\n",
    "image_dir = Path('/common/users/vk405/EnergyLab/Data')\n",
    "\n",
    "class Dset(Dataset):\n",
    "    def __init__(self,split= 'train',data_dir='/common/home/vk405/Projects/EnergyLab/Solar-forcast/Data/',\n",
    "    image_dir='/common/users/vk405/EnergyLab/Data/ProcData'):\n",
    "        self.split = split\n",
    "\n",
    "        #hardcoded dir locs \n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.image_dir = Path(image_dir) \n",
    "        \n",
    "        base_df = pd.read_csv(f'{self.data_dir}/tgtimgs.csv')\n",
    "        base_df.loc[base_df.index[base_df['Target']>=1250.0],'Target'] = 1250.0\n",
    "        \n",
    "        trn_df = pd.DataFrame(base_df.loc[pd.to_datetime(base_df['Date']).dt.year <= 2014])\n",
    "        self.scalar = MinMaxScaler().fit(trn_df['Target'].values.reshape(-1,1))\n",
    "        #splits are hardcoded as per the original paper\n",
    "        if self.split == 'train':\n",
    "            self.df = pd.DataFrame(base_df.loc[pd.to_datetime(base_df['Date']).dt.year <= 2014])\n",
    "            \n",
    "        elif self.split == 'valid':\n",
    "            self.df = pd.DataFrame(base_df.loc[pd.to_datetime(base_df['Date']).dt.year == 2015])\n",
    "            \n",
    "        else:\n",
    "            self.df = pd.DataFrame(base_df.loc[pd.to_datetime(base_df['Date']).dt.year > 2015])\n",
    "        #set an upper-threshold.\n",
    "        self.df.loc[self.df.index[self.df['Target']>=1250.0],'Target'] = 1250.0\n",
    "        #self.df[self.df['Target']>=1300.0]['Target'] = 1300.0\n",
    "        rescaled = self.scalar.transform(self.df['Target'].values.reshape(-1,1))\n",
    "        self.df['Target'] = np.squeeze(rescaled)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self,ind):\n",
    "        try:\n",
    "            imgs = sorted(eval(self.df.iloc[ind]['Imgs']),key = lambda x: int(x.split('/')[-1].split('.')[0]))\n",
    "            target = self.df.iloc[ind]['Target']\n",
    "            #image_dir = Path('/common/users/vk405/EnergyLab/Data/ProcData') \n",
    "            proc_imgs = [self.image_dir/(x.split('/')[-1].split('.')[0]+'.joblib') for x in imgs]\n",
    "            proc_arrays = [joblib.load(ele) for ele in proc_imgs]\n",
    "        except:\n",
    "            print(ind)\n",
    "            \n",
    "        return (np.concatenate(proc_arrays,-1).reshape(-1,256,256))/255.0,target\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def SCNN(n_ch=6):\n",
    "    \n",
    "    model = []\n",
    "    model.append(nn.Conv2d(in_channels=n_ch, out_channels=64, kernel_size=(3, 3), padding=\"same\"))\n",
    "    model.append(nn.ReLU())\n",
    "    model.append(nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), padding=\"same\"))\n",
    "    model.append(nn.ReLU())\n",
    "    model.append(nn.MaxPool2d((2,2),stride=(2,2)))\n",
    "    \n",
    "    model.append(nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), padding=\"same\"))\n",
    "    model.append(nn.ReLU())\n",
    "    model.append(nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), padding=\"same\"))\n",
    "    model.append(nn.ReLU())\n",
    "    model.append(nn.MaxPool2d((2,2),stride=(2,2)))\n",
    "\n",
    "    model.append(nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3, 3), padding=\"same\"))\n",
    "    model.append(nn.ReLU())\n",
    "    model.append(nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3, 3), padding=\"same\"))\n",
    "    model.append(nn.ReLU())\n",
    "    model.append(nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3, 3), padding=\"same\"))\n",
    "    model.append(nn.ReLU())\n",
    "    model.append(nn.MaxPool2d((2,2),stride=(2,2)))\n",
    "\n",
    "    model.append(nn.Conv2d(in_channels=256, out_channels=512, kernel_size=(3, 3), padding=\"same\"))\n",
    "    model.append(nn.ReLU())\n",
    "    model.append(nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), padding=\"same\"))\n",
    "    model.append(nn.ReLU())\n",
    "    #model.append(nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), padding=\"same\"))\n",
    "    #model.append(nn.ReLU())\n",
    "    model.append(nn.MaxPool2d((2,2),stride=(2,2)))\n",
    "\n",
    "    model.append(nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), padding=\"same\"))\n",
    "    model.append(nn.ReLU())\n",
    "    model.append(nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), padding=\"same\"))\n",
    "    model.append(nn.ReLU())\n",
    "    #model.append(nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), padding=\"same\"))\n",
    "    #model.append(nn.ReLU())\n",
    "    model.append(nn.MaxPool2d((2,2),stride=(2,2)))\n",
    "\n",
    "    model.append(nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), padding=\"same\"))\n",
    "    model.append(nn.ReLU())\n",
    "    model.append(nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), padding=\"same\"))\n",
    "    model.append(nn.ReLU())\n",
    "    model.append(nn.MaxPool2d((2,2),stride=(2,2)))\n",
    "\n",
    "\n",
    "\n",
    "    return nn.Sequential(*model)\n",
    "\n",
    "\n",
    "class CombModel(pl.LightningModule):\n",
    "    def __init__(self,hparams,dset=None):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        \n",
    "        if hparams.framecnt != 2:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        #get the complete model\n",
    "        base_model = [SCNN(hparams.framecnt*3)]\n",
    "        base_model.extend([nn.Flatten(),nn.Dropout(hparams.dropoutp),nn.Linear(8192,256)])\n",
    "        self.prenet = nn.Sequential(*base_model)\n",
    "        #nn.Dropout(hparams.dropoutp),nn.Linear(256,1)\n",
    "        self.autoregressnet = nn.Sequential(nn.Linear(hparams.lkback,hparams.lkback//2),nn.ReLU())\n",
    "\n",
    "        self.combined_net = nn.Sequential(nn.Dropout(hparams.dropoutp),nn.Linear(256+hparams.lkback//2,1))\n",
    "\n",
    "    def forward(self,x,lkbk_vals):\n",
    "        #keep this for inference\n",
    "        image_out = self.prenet(x)\n",
    "        regres_out = self.autoregressnet(lkbk_vals)\n",
    "        out  = self.combined_net(torch.concat([image_out,regres_out],dim=-1))\n",
    "        return out\n",
    "\n",
    "    \n",
    "    def training_step(self,batch,batch_idx):\n",
    "        #for training\n",
    "        x,x_r,y = batch\n",
    "        y_hat = torch.squeeze(self.forward(x.float(),x_r.float()))\n",
    "\n",
    "        loss = F.l1_loss(y_hat,y.float())\n",
    "        self.log(\"train_loss\",loss,on_step=True)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        #for training\n",
    "        x,x_r,y = batch\n",
    "        y_hat = torch.squeeze(self.forward(x.float(),x_r.float()))\n",
    "\n",
    "\n",
    "        loss = F.l1_loss(y_hat,y.float())\n",
    "        self.log(\"val_loss\",loss,on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr if 'lr' in self.hparams else 1e-5\n",
    "        #lr=0.00001, beta_1=0.9, beta_2=0.999, decay=0.0, amsgrad=False\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr,betas=(0.9,0.999),amsgrad=False)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#new dataset\n",
    "\n",
    "class Dset(Dataset):\n",
    "    def __init__(self,split= 'train',data_dir='/common/home/vk405/Projects/EnergyLab/Solar-forcast/Data/',\n",
    "    image_dir='/common/users/vk405/EnergyLab/Data/ProcData',lkback=10):\n",
    "        self.split = split\n",
    "\n",
    "        #hardcoded dir locs \n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.image_dir = Path(image_dir) \n",
    "        self.lkback = lkback\n",
    "        \n",
    "        base_df = pd.read_csv(f'{self.data_dir}/tgtimgs.csv')\n",
    "        base_df.loc[base_df.index[base_df['Target']>=1250.0],'Target'] = 1250.0\n",
    "        \n",
    "        trn_df = pd.DataFrame(base_df.loc[pd.to_datetime(base_df['Date']).dt.year <= 2014])\n",
    "        self.scalar = MinMaxScaler().fit(trn_df['Target'].values.reshape(-1,1))\n",
    "\n",
    "        trn_msi = joblib.load(self.data_dir /'trn_msi.joblib')\n",
    "        trn_msi[trn_msi>=1250] = 1250.0\n",
    "        self.msi_scalar = MinMaxScaler().fit(trn_msi.reshape(-1,1))\n",
    "        #splits are hardcoded as per the original paper\n",
    "        if self.split == 'train':\n",
    "            self.df = pd.DataFrame(base_df.loc[pd.to_datetime(base_df['Date']).dt.year <= 2014])\n",
    "            self.msi = joblib.load(self.data_dir /'trn_msi.joblib')\n",
    "        \n",
    "            \n",
    "        elif self.split == 'valid':\n",
    "            self.df = pd.DataFrame(base_df.loc[pd.to_datetime(base_df['Date']).dt.year == 2015])\n",
    "            self.msi = joblib.load(self.data_dir /'vld_msi.joblib')\n",
    "            \n",
    "        else:\n",
    "            self.df = pd.DataFrame(base_df.loc[pd.to_datetime(base_df['Date']).dt.year > 2015])\n",
    "            self.msi = joblib.load(self.data_dir /'tst_msi.joblib')\n",
    "        #set an upper-threshold.\n",
    "        self.df.loc[self.df.index[self.df['Target']>=1250.0],'Target'] = 1250.0\n",
    "        #self.df[self.df['Target']>=1300.0]['Target'] = 1300.0\n",
    "        rescaled = self.scalar.transform(self.df['Target'].values.reshape(-1,1))\n",
    "        self.df['Target'] = np.squeeze(rescaled)\n",
    "        self.msi = np.squeeze(self.msi_scalar.transform(self.msi.reshape(-1,1)))\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self,ind):\n",
    "        try:\n",
    "            imgs = sorted(eval(self.df.iloc[ind]['Imgs']),key = lambda x: int(x.split('/')[-1].split('.')[0]))\n",
    "            target = self.df.iloc[ind]['Target']\n",
    "            #image_dir = Path('/common/users/vk405/EnergyLab/Data/ProcData') \n",
    "            proc_imgs = [self.image_dir/(x.split('/')[-1].split('.')[0]+'.joblib') for x in imgs]\n",
    "            proc_arrays = [joblib.load(ele) for ele in proc_imgs]\n",
    "            proc_lkback = self.msi[ind*self.lkback:ind*self.lkback+10]\n",
    "        except:\n",
    "            print(ind)\n",
    "            \n",
    "        return (np.concatenate(proc_arrays,-1).reshape(-1,256,256))/255.0,proc_lkback,target\n",
    "\n",
    "\n",
    "\n",
    "def run_model(cfg):\n",
    "    pl.seed_everything(cfg.seed)\n",
    "    dir = cfg.artifacts_loc\n",
    "    version = str(cfg.version)\n",
    "    logger_list = get_loggers(dir, version,cfg.loggers)\n",
    "    cbs = []\n",
    "    if \"early_stop\" in cfg.cbs:\n",
    "        #? does'nt really work atm\n",
    "        #params = cfg.model.cbs.early_stop\n",
    "        earlystopcb = EarlyStopping(monitor='val_loss',mode=\"min\",patience=3,verbose=False)\n",
    "        cbs.append(earlystopcb)\n",
    "    if \"checkpoint\" in cfg.cbs:\n",
    "        store_path = dir + \"ckpts/\" + str(cfg.version) + \"/\"\n",
    "        isExist = os.path.exists(store_path)\n",
    "        if not isExist:\n",
    "            os.makedirs(store_path)\n",
    "        fname = \"{epoch}-{val_loss:.2f}\"\n",
    "        params = cfg.checkpoint\n",
    "        checkptcb = ModelCheckpoint(**params, dirpath=store_path, filename=fname)\n",
    "        cbs.append(checkptcb)\n",
    "\n",
    "    #wandb.init(project=\"solarforecast\", config=cfg)\n",
    "    if cfg.mode == 'train':\n",
    "        trn_fdata = Dset(data_dir=cfg.data_dir,split=cfg.mode,image_dir= cfg.image_dir,\\\n",
    "               lkback=cfg.lkback)\n",
    "        vld_fdata = Dset(data_dir=cfg.data_dir,split='valid',image_dir= cfg.image_dir,lkback=cfg.lkback)\n",
    "\n",
    "        val_loader = DataLoader(vld_fdata,\\\n",
    "            batch_size=cfg.batch_size,shuffle=False,num_workers=4,pin_memory=True)\n",
    "        train_loader = DataLoader(trn_fdata,\\\n",
    "            batch_size=cfg.batch_size,shuffle=True,num_workers=4,pin_memory=True)\n",
    "            \n",
    "        hparams = cfg    \n",
    "        net = CombModel(hparams)\n",
    "        trainer = pl.Trainer(\n",
    "            logger=logger_list,callbacks=cbs, gpus=[0,1,2,3],deterministic=True, **cfg.trainer\n",
    "        )\n",
    "        trainer.fit(net,train_dataloaders=train_loader,val_dataloaders=val_loader)\n",
    "        return trainer\n",
    "        #trainer.tune(net,train_loader)\n",
    "            \n",
    "    else:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trndset = Dset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                                             2012-01-01\n",
       "MST                                                   07:40\n",
       "Imgs      ['/common/users/vk405/EnergyLab/Data/20120101/...\n",
       "Target                                              0.02127\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trndset.df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_svd = pd.read_csv(os.path.join(data_dir, 'SRRL_measurement_timeseries.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>MST</th>\n",
       "      <th>GHI</th>\n",
       "      <th>DateTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>07:25</td>\n",
       "      <td>10.991</td>\n",
       "      <td>2012-01-01 07:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>07:26</td>\n",
       "      <td>12.329</td>\n",
       "      <td>2012-01-01 07:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>07:27</td>\n",
       "      <td>13.698</td>\n",
       "      <td>2012-01-01 07:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>07:28</td>\n",
       "      <td>15.007</td>\n",
       "      <td>2012-01-01 07:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>07:29</td>\n",
       "      <td>15.516</td>\n",
       "      <td>2012-01-01 07:29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date    MST     GHI          DateTime\n",
       "0  2012-01-01  07:25  10.991  2012-01-01 07:25\n",
       "1  2012-01-01  07:26  12.329  2012-01-01 07:26\n",
       "2  2012-01-01  07:27  13.698  2012-01-01 07:27\n",
       "3  2012-01-01  07:28  15.007  2012-01-01 07:28\n",
       "4  2012-01-01  07:29  15.516  2012-01-01 07:29"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_svd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "smpl = ' '.join(['2012-01-01','07:40'])\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_svd[df_svd['DateTime'] == smpl].index.tolist()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(all_data,ext_for,lkbk=10):\n",
    "    ext_vals = []\n",
    "    for i in tqdm(range(len(ext_for))):\n",
    "        dt = ' '.join([ext_for.iloc[i]['Date'],ext_for.iloc[i]['MST']])\n",
    "        ind = all_data[all_data['DateTime'] == dt].index.tolist()[0]\n",
    "        ext_vals.append(all_data.iloc[ind-lkbk+1:ind+1]['GHI'].values)\n",
    "    return np.concatenate(ext_vals)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 3925/58531 [05:40<1:18:51, 11.54it/s]"
     ]
    }
   ],
   "source": [
    "trn_msi = extract(df_svd,trndset.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(trn_msi,data_dir/'trn_msi.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16.033, 17.699, 19.783, 19.342, 18.155, 18.422, 18.262, 17.904,\n",
       "       18.133, 18.962, 20.072])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vlddset = Dset(split='valid')\n",
    "\n",
    "vld_msi = extract(df_svd,vlddset.df)\n",
    "joblib.dump(vld_msi,data_dir/'vld_msi.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstdset = Dset(split='test')\n",
    "\n",
    "tst_msi = extract(df_svd,tstdset.df)\n",
    "joblib.dump(tst_msi,data_dir/'tst_msi.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_msi = joblib.load(data_dir/'trn_msi.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_msi[trn_msi>=1250] = 1250.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = MinMaxScaler().fit(trn_msi.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.96]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.transform([[1200]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "585310"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn_msi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17.699, 19.783, 19.342, 18.155, 18.422, 18.262, 17.904, 18.133,\n",
       "       18.962, 20.072])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_msi[0*10:0*10+10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1587.36, 0.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_msi.max(),trn_msi.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a joint model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 6, 256, 256]) torch.Size([8, 10]) torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "\n",
    "dset = Dset()\n",
    "dloader = DataLoader(dset,\\\n",
    "            batch_size=8,shuffle=False)\n",
    "x,x_r,y = None,None,None\n",
    "for batch in dloader:\n",
    "    x,x_r,y = batch\n",
    "    print(x.shape,x_r.shape,y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "combmodel = CombModel(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = combmodel(x.float(),x_r.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from argparse import Namespace\n",
    "    cfg = Namespace(\n",
    "        version = 'combtrail',\n",
    "        artifacts_loc = \"/common/home/vk405/Projects/EnergyLab/Solar-forcast/artifacts/\",\n",
    "        data_dir = \"/common/home/vk405/Projects/EnergyLab/Solar-forcast/Data/\",\n",
    "        image_dir = \"/common/users/vk405/EnergyLab/Data/ProcData/\",\n",
    "        mode = 'train',\n",
    "        loggers = [\"csv\"],\n",
    "        seed = 0,\n",
    "        cbs = [\"checkpoint\"],\n",
    "        trainer = {'log_every_n_steps': 50,\n",
    "        'max_epochs': 40},\n",
    "        checkpoint = {\"save_top_k\": 5,\n",
    "        \"monitor\": \"val_loss\",\"mode\":\"min\"},\n",
    "        dropoutp=0.2,\n",
    "        framecnt=2,\n",
    "        lr = 1.5e-5,\n",
    "        batch_size=64,\n",
    "        lkback = 10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model,loader):\n",
    "    preds_lis = []\n",
    "    truelabels = []\n",
    "    with torch.no_grad():\n",
    "        for x,x_r,y in tqdm(test_loader):\n",
    "            preds = model(x.float(),x_r.float())\n",
    "            preds_lis.append(preds.squeeze().cpu().numpy())\n",
    "            truelabels.append(y.cpu().numpy())\n",
    "    return np.concatenate(preds_lis,axis=0),np.concatenate(truelabels,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference\n",
    "\n",
    "#trained model\n",
    "net = CombModel(cfg)\n",
    "weight_loc = '/common/home/vk405/Projects/EnergyLab/Solar-forcast/artifacts/ckpts/combtrail/epoch=39-val_loss=0.08.ckpt'\n",
    "trnd_net = net.load_from_checkpoint(weight_loc)\n",
    "\n",
    "test_dset = Dset(split= 'test')\n",
    "test_loader = DataLoader(test_dset,\\\n",
    "            batch_size=64,shuffle=False,num_workers=4,pin_memory=True)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 436/436 [39:19<00:00,  5.41s/it]\n"
     ]
    }
   ],
   "source": [
    "p,g = infer(trnd_net,test_loader)\n",
    "infered_vals = {'Date':test_dset.df['Date'].values,'MST':test_dset.df['MST'].values,\n",
    "    'pred':p,'ground':g,'Target':test_dset.df['Target'].values}\n",
    "pd.DataFrame(infered_vals).to_csv(data_dir/'infered_vals_comb.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_preds = pd.DataFrame(infered_vals)\n",
    "#tst_preds['abs_error'] = np.abs(tst_preds['ground']-tst_preds['pred'])\n",
    "tst_preds['error'] = tst_preds['pred']-tst_preds['ground']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "small,med = tst_preds['Target'].quantile(0.33),tst_preds['Target'].quantile(0.66)\n",
    "\n",
    "cloudy = tst_preds[tst_preds['Target']<=small]\n",
    "normal = tst_preds[(tst_preds['Target']>small) & (tst_preds['Target']<=med)]\n",
    "sunny = tst_preds[(tst_preds['Target']>med)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.11896533868615822, 0.0629888550466521)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# yes cloudy is bad(can do hypothesis testing for better confirmation.)\n",
    "cloudy['error'].abs().mean(),sunny['error'].abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.11896533868615822, 0.08061535536523695)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloudy['error'].abs().mean(),normal['error'].abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#over-prediction vs underprediction(looks not so convincing.)\n",
    "over_preds = tst_preds[tst_preds['error']>0.0]\n",
    "under_preds = tst_preds[tst_preds['error']<=0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.09761838431070016, 0.06821019487593272)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "over_preds['error'].abs().mean(),under_preds['error'].abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08727852826239703"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "tst_preds['error'].abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ba0eaf5009993b745d4aa7d6cba132d7a7c20d53b6841ddae3db28e24457bb23"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('Crossmdl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
